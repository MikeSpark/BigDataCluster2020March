{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark"
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "datafile = \"/user/root/AdultCensusIncome.csv\"\r\n",
                "\r\n",
                "#Read the data to a spark data frame.\r\n",
                "data_all = spark.read.format('csv').options(header='true', inferSchema='true', ignoreLeadingWhiteSpace='true', ignoreTrailingWhiteSpace='true').load(datafile)\r\n",
                "print(\"Number of rows: {},  Number of coulumns : {}\".format(data_all.count(), len(data_all.columns)))\r\n",
                "data_all.printSchema() \r\n",
                "\r\n",
                "#Replace \"-\" with \"_\" in column names\r\n",
                "columns_new = [col.replace(\"-\", \"_\") for col in data_all.columns]\r\n",
                "data_all = data_all.toDF(*columns_new)\r\n",
                "data_all.printSchema()"
            ],
            "metadata": {
                "azdata_cell_guid": "705e70bc-cffa-4b31-99fe-ccbf0544bef3"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Starting Spark application\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1583908949111_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://52.139.168.178:30443/gateway/default/yarn/proxy/application_1583908949111_0003/\">Link</a></td><td><a target=\"_blank\" href=\"https://52.139.168.178:30443/gateway/default/yarn/container/container_1583908949111_0003_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "c32d8305ca1b47b6972376ab076a0b66"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "f90b178f044d47a9a274bb4fe8cd1ffc"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "Number of rows: 32561,  Number of coulumns : 15\nroot\n |-- age: integer (nullable = true)\n |-- workclass: string (nullable = true)\n |-- fnlwgt: integer (nullable = true)\n |-- education: string (nullable = true)\n |-- education-num: integer (nullable = true)\n |-- marital-status: string (nullable = true)\n |-- occupation: string (nullable = true)\n |-- relationship: string (nullable = true)\n |-- race: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- capital-gain: integer (nullable = true)\n |-- capital-loss: integer (nullable = true)\n |-- hours-per-week: integer (nullable = true)\n |-- native-country: string (nullable = true)\n |-- income: string (nullable = true)\n\nroot\n |-- age: integer (nullable = true)\n |-- workclass: string (nullable = true)\n |-- fnlwgt: integer (nullable = true)\n |-- education: string (nullable = true)\n |-- education_num: integer (nullable = true)\n |-- marital_status: string (nullable = true)\n |-- occupation: string (nullable = true)\n |-- relationship: string (nullable = true)\n |-- race: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- capital_gain: integer (nullable = true)\n |-- capital_loss: integer (nullable = true)\n |-- hours_per_week: integer (nullable = true)\n |-- native_country: string (nullable = true)\n |-- income: string (nullable = true)",
                    "output_type": "stream"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "#Basic data exploration\r\n",
                "\r\n",
                "##1. Sub set the data and print some important columns\r\n",
                "print(\"Select few columns to see the data\")\r\n",
                "data_all.select(['income','age','hours_per_week', 'education']).show(10)\r\n",
                "\r\n",
                "## Find the number of distict values\r\n",
                "print(\"Number of distinct values for income\")\r\n",
                "ds_sub = data_all.select('income').distinct()\r\n",
                "ds_sub.show()\r\n",
                "\r\n",
                "##Add a numberic column(income_code) derived from income column\r\n",
                "print(\"Added numeric column(income_code) derived from income column\")\r\n",
                "from pyspark.sql.functions import expr\r\n",
                "\r\n",
                "df_new = data_all.withColumn(\"income_code\", expr(\"case \\\r\n",
                "                                            when income like '%<=50K%' then 0 \\\r\n",
                "                                            when income like '%>50K%' then 1 \\\r\n",
                "                                            else 2 end \"))\r\n",
                "\r\n",
                "df_new.select(['income', 'age', 'hours_per_week', 'education', 'income_code']).show(10)\r\n",
                "\r\n",
                "##Summary  statistical operations on dataframe\r\n",
                "print(\"Print a statistical summary of a few columns\")\r\n",
                "df_new.select(['income','age','hours_per_week', 'education','income_code']).describe().show()\r\n",
                "\r\n",
                "print(\"Calculate Co variance between a few columns to understand features to use\")\r\n",
                "mycov = df_new.stat.cov('income_code','hours_per_week')\r\n",
                "print(\"Covariance between income and hours_per_week is\", round(mycov,1))\r\n",
                "\r\n",
                "mycov = df_new.stat.cov('income_code','age')\r\n",
                "print(\"Covariance between income and age is\", round(mycov,1))"
            ],
            "metadata": {
                "azdata_cell_guid": "7f520d9d-7960-45c4-bb99-a5731dc8db8b"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "1f3ddd0af1f144de8693d8443072d255"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "Select few columns to see the data\n+------+---+--------------+---------+\n|income|age|hours_per_week|education|\n+------+---+--------------+---------+\n| <=50K| 39|            40|Bachelors|\n| <=50K| 50|            13|Bachelors|\n| <=50K| 38|            40|  HS-grad|\n| <=50K| 53|            40|     11th|\n| <=50K| 28|            40|Bachelors|\n| <=50K| 37|            40|  Masters|\n| <=50K| 49|            16|      9th|\n|  >50K| 52|            45|  HS-grad|\n|  >50K| 31|            50|  Masters|\n|  >50K| 42|            40|Bachelors|\n+------+---+--------------+---------+\nonly showing top 10 rows\n\nNumber of distinct values for income\n+------+\n|income|\n+------+\n| <=50K|\n|  >50K|\n+------+\n\nAdded numeric column(income_code) derived from income column\n+------+---+--------------+---------+-----------+\n|income|age|hours_per_week|education|income_code|\n+------+---+--------------+---------+-----------+\n| <=50K| 39|            40|Bachelors|          0|\n| <=50K| 50|            13|Bachelors|          0|\n| <=50K| 38|            40|  HS-grad|          0|\n| <=50K| 53|            40|     11th|          0|\n| <=50K| 28|            40|Bachelors|          0|\n| <=50K| 37|            40|  Masters|          0|\n| <=50K| 49|            16|      9th|          0|\n|  >50K| 52|            45|  HS-grad|          1|\n|  >50K| 31|            50|  Masters|          1|\n|  >50K| 42|            40|Bachelors|          1|\n+------+---+--------------+---------+-----------+\nonly showing top 10 rows\n\nPrint a statistical summary of a few columns\n+-------+------+------------------+------------------+------------+-------------------+\n|summary|income|               age|    hours_per_week|   education|        income_code|\n+-------+------+------------------+------------------+------------+-------------------+\n|  count| 32561|             32561|             32561|       32561|              32561|\n|   mean|  null| 38.58164675532078|40.437455852092995|        null| 0.2408095574460244|\n| stddev|  null|13.640432553581356|12.347428681731838|        null|0.42758148856469247|\n|    min| <=50K|                17|                 1|        10th|                  0|\n|    max|  >50K|                90|                99|Some-college|                  1|\n+-------+------+------------------+------------------+------------+-------------------+\n\nCalculate Co variance between a few columns to understand features to use\nCovariance between income and hours_per_week is 1.2\nCovariance between income and age is 1.4",
                    "output_type": "stream"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": [
                "# Choose feature columns and the label column.\r\n",
                "label = \"income\"\r\n",
                "xvars = [\"age\", \"hours_per_week\", 'education'] #numeric and string\r\n",
                "\r\n",
                "print(\"label = {}\".format(label))\r\n",
                "print(\"features = {}\".format(xvars))\r\n",
                "\r\n",
                "#Check label counts to check data bias\r\n",
                "print(\"Count of rows that are <=50K\", data_all[data_all.income==\"<=50K\"].count())\r\n",
                "print(\"Count of rows that are >50K\", data_all[data_all.income==\">50K\"].count())\r\n",
                "\r\n",
                "\r\n",
                "select_cols = xvars\r\n",
                "select_cols.append(label)\r\n",
                "data = data_all.select(select_cols)"
            ],
            "metadata": {
                "azdata_cell_guid": "94f4061d-b662-4633-92a9-b33a5d91b758"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "6a82821fff34425e8e4e784d0c1da062"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "label = income\nfeatures = ['age', 'hours_per_week', 'education']\nCount of rows that are <=50K 24720\nCount of rows that are >50K 7841",
                    "output_type": "stream"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": [
                "train, test = data.randomSplit([0.75, 0.25], seed=123)\r\n",
                "\r\n",
                "print(\"train ({}, {})\".format(train.count(), len(train.columns)))\r\n",
                "print(\"test ({}, {})\".format(test.count(), len(test.columns)))\r\n",
                "\r\n",
                "train_data_path = \"/spark_ml/AdultCensusIncomeTrain\"\r\n",
                "test_data_path = \"/spark_ml/AdultCensusIncomeTest\"\r\n",
                "\r\n",
                "train.write.mode('overwrite').orc(train_data_path)\r\n",
                "test.write.mode('overwrite').orc(test_data_path)\r\n",
                "print(\"train and test datasets saved to {} and {}\".format(train_data_path, test_data_path))"
            ],
            "metadata": {
                "azdata_cell_guid": "4f1da7af-eb64-4aac-95df-dc257b8eceb5"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "5467e34f801d4a919004e534790554d1"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "train (24469, 4)\ntest (8092, 4)\ntrain and test datasets saved to /spark_ml/AdultCensusIncomeTrain and /spark_ml/AdultCensusIncomeTest",
                    "output_type": "stream"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml import Pipeline, PipelineModel\r\n",
                "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\r\n",
                "from pyspark.ml.classification import LogisticRegression\r\n",
                "\r\n",
                "reg = 0.1\r\n",
                "print(\"Using LogisticRegression model with Regularization Rate of {}.\".format(reg))\r\n",
                "\r\n",
                "# create a new Logistic Regression model.\r\n",
                "lr = LogisticRegression(regParam=reg)\r\n",
                "\r\n",
                "dtypes = dict(train.dtypes)\r\n",
                "dtypes.pop(label)\r\n",
                "\r\n",
                "si_xvars = []\r\n",
                "ohe_xvars = []\r\n",
                "featureCols = []\r\n",
                "for idx,key in enumerate(dtypes):\r\n",
                "    if dtypes[key] == \"string\":\r\n",
                "        featureCol = \"-\".join([key, \"encoded\"])\r\n",
                "        featureCols.append(featureCol)\r\n",
                "        \r\n",
                "        tmpCol = \"-\".join([key, \"tmp\"])\r\n",
                "        si_xvars.append(StringIndexer(inputCol=key, outputCol=tmpCol, handleInvalid=\"skip\")) #, handleInvalid=\"keep\"\r\n",
                "        ohe_xvars.append(OneHotEncoderEstimator(inputCols=[tmpCol], outputCols=[featureCol]))\r\n",
                "    else:\r\n",
                "        featureCols.append(key)\r\n",
                "\r\n",
                "# string-index the label column into a column named \"label\"\r\n",
                "si_label = StringIndexer(inputCol=label, outputCol='label')\r\n",
                "\r\n",
                "# assemble the encoded feature columns in to a column named \"features\"\r\n",
                "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\r\n",
                "\r\n",
                "\r\n",
                "stages = []\r\n",
                "stages.extend(si_xvars)\r\n",
                "stages.extend(ohe_xvars)\r\n",
                "stages.append(si_label)\r\n",
                "stages.append(assembler)\r\n",
                "stages.append(lr)\r\n",
                "pipe = Pipeline(stages=stages)\r\n",
                "print(\"Pipeline Created\")\r\n",
                "\r\n",
                "model = pipe.fit(train)\r\n",
                "print(\"Model Trained\")\r\n",
                "print(\"Model is \", model)\r\n",
                "print(\"Model Stages\", model.stages)"
            ],
            "metadata": {
                "azdata_cell_guid": "5aa316d2-ef50-4f4e-bc25-0bfc971a41a5"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "99b7ed25c0d840f3876f5eddf4b94a53"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "Using LogisticRegression model with Regularization Rate of 0.1.\nPipeline Created\nModel Trained\nModel is  PipelineModel_697f2c47874d\nModel Stages [StringIndexer_eb4260535b16, OneHotEncoderEstimator_9ef3e47fe66c, StringIndexer_6c9c932623c1, VectorAssembler_d4a6d1c4a6d3, LogisticRegressionModel: uid = LogisticRegression_fed6a208d061, numClasses = 2, numFeatures = 17]",
                    "output_type": "stream"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml.evaluation import BinaryClassificationEvaluator\r\n",
                "\r\n",
                "# make prediction\r\n",
                "pred = model.transform(test)\r\n",
                "\r\n",
                "# evaluate. note only 2 metrics are supported out of the box by Spark ML.\r\n",
                "bce = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')\r\n",
                "au_roc = bce.setMetricName('areaUnderROC').evaluate(pred)\r\n",
                "au_prc = bce.setMetricName('areaUnderPR').evaluate(pred)\r\n",
                "\r\n",
                "print(\"Area under ROC: {}\".format(au_roc))\r\n",
                "print(\"Area Under PR: {}\".format(au_prc))\r\n",
                "\r\n",
                "pred[pred.prediction==1.0][pred.income,pred.label,pred.prediction].show()"
            ],
            "metadata": {
                "azdata_cell_guid": "39386faf-0847-4d2d-839b-e1f652474d19"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "a7d7af29749540029476d9069bd5e232"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "Area under ROC: 0.7964496884726682\nArea Under PR: 0.5358180243123482\n+------+-----+----------+\n|income|label|prediction|\n+------+-----+----------+\n| <=50K|  0.0|       1.0|\n|  >50K|  1.0|       1.0|\n| <=50K|  0.0|       1.0|\n| <=50K|  0.0|       1.0|\n|  >50K|  1.0|       1.0|\n| <=50K|  0.0|       1.0|\n|  >50K|  1.0|       1.0|\n| <=50K|  0.0|       1.0|\n| <=50K|  0.0|       1.0|\n|  >50K|  1.0|       1.0|\n| <=50K|  0.0|       1.0|\n|  >50K|  1.0|       1.0|\n| <=50K|  0.0|       1.0|\n|  >50K|  1.0|       1.0|\n| <=50K|  0.0|       1.0|\n| <=50K|  0.0|       1.0|\n| <=50K|  0.0|       1.0|\n|  >50K|  1.0|       1.0|\n| <=50K|  0.0|       1.0|\n|  >50K|  1.0|       1.0|\n+------+-----+----------+\nonly showing top 20 rows",
                    "output_type": "stream"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                "\r\n",
                "model_name = \"AdultCensus.mml\"\r\n",
                "model_fs = \"/spark_ml/\" + model_name\r\n",
                "\r\n",
                "model.write().overwrite().save(model_fs)\r\n",
                "print(\"saved model to {}\".format(model_fs))\r\n",
                "\r\n",
                "# load the model file and check its same as the in-memory model\r\n",
                "model2 = PipelineModel.load(model_fs)\r\n",
                "assert str(model2) == str(model)\r\n",
                "print(\"Successfully loaded from {}\".format(model_fs))"
            ],
            "metadata": {
                "azdata_cell_guid": "ffe984c3-0de0-4e40-a612-792837c2409c"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "726f4391c23842b1802d79fe148177a8"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "saved model to /spark_ml/AdultCensus.mml\nSuccessfully loaded from /spark_ml/AdultCensus.mml",
                    "output_type": "stream"
                }
            ],
            "execution_count": 9
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
                "# serialize the model to a zip file in JSON format\n",
                "model_name_export = \"adult_census_pipeline.zip\"\n",
                "model_name_path = os.getcwd()\n",
                "model_file = os.path.join(model_name_path, model_name_export)\n",
                "# remove an old model file, if needed.\n",
                "try:\n",
                "    os.remove(model_file)\n",
                "except OSError:\n",
                "    pass\n",
                "model_file_path = \"jar:file:{}\".format(model_file)\n",
                "model.serializeToBundle(model_file_path, model.transform(train))\n",
                "print(\"persist the mleap bundle from local to hdfs\")\n",
                "from subprocess import Popen, PIPE\n",
                "proc = Popen([\"hadoop\", \"fs\", \"-put\", \"-f\", model_file, os.path.join(\"/spark_ml\", model_name_export)], stdout=PIPE, stderr=PIPE)\n",
                "s_output, s_err = proc.communicate()"
            ],
            "metadata": {
                "azdata_cell_guid": "74c3af0d-ebbf-414c-8c70-bd89d956f084"
            },
            "outputs": [
{
    "data": {
        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
        "application/vnd.jupyter.widget-view+json": {
            "version_major": 2,
            "version_minor": 0,
            "model_id": "3f92d6fc72414b5fa24c5092759b5944"
        }
    },
    "metadata": {},
    "output_type": "display_data"
}, {
    "name": "stderr",
    "text": "An error was encountered:\n[Errno 2] No such file or directory: 'hadoop'\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/subprocess.py\", line 947, in __init__\n    restore_signals, start_new_session)\n  File \"/usr/lib/python3.5/subprocess.py\", line 1551, in _execute_child\n    raise child_exception_type(errno_num, err_msg)\nFileNotFoundError: [Errno 2] No such file or directory: 'hadoop'\n\n",
    "output_type": "stream"
}
],
            "execution_count": 10
        }
    ]
}